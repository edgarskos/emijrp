\documentclass[11pt,twocolumn]{article}
\setlength{\columnsep}{0.5cm}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{natbib}

\title{\vspace{-15mm}
	\fontsize{24pt}{10pt}\selectfont
	%\textbf{WikiTeam: collaborative preservation of wikis}
	\textbf{WikiTeam: preservación colaborativa de wikis}
	}
\author{
	\large
	\textsc{Emilio J. Rodríguez-Posada} \\
	\normalsize	Private \\
	\normalsize	\href{mailto:emijrp@gmail.com}{emijrp@gmail.com}
	\vspace{-5mm}
	}
\date{}


\begin{document}


\twocolumn[
  \begin{@twocolumnfalse}

    \maketitle

\begin{abstract}
  Los internautas tienen cada vez un papel más relevante en la generación del contenido de los sitios web. Existen iniciativas y soluciones para la preservación digital de la web, incluso con largo recorrido como Internet Archive, pero son ineficientes y padecen dificultades a la hora de preservar contenido creado por los usuarios en redes sociales y wikis. En este artículo analizamos los problemas existentes a la hora de preservar wikis, la ausencia de herramientas para realizar esta labor y presentamos y evaluamos la solución que hemos construido, llamada WikiTeam. El software desarrollado en WikiTeam es un conjunto de herramientas para preservar wikis, hasta el momento MediaWiki. Tras ponerlo en práctica hemos logrado extraer los textos, historiales, imágenes y metadatos de más de 4.500 wikis de toda la red. Con la experiencia recabada planeamos no solo ampliar el número de wikis preservados sino expandirnos a otros motores wiki. Todo el contenido recuperado representa un cúmulo enorme de datasets de la wikiesfera, con un incalculable valor tanto histórico como para la investigación de estas comunidades wiki.
  \\
  \\
  \textbf{Palabras clave:} web digital preservation, social web archiving, archiving applications and systems

\end{abstract}

  \end{@twocolumnfalse}
  ]

\section{Introducción}

La preservación web consiste en la recuperación, almacenamiento y conservación de contenido de la World Wide Web con fines históricos y de investigación. Desde que en 1996 se fundó Internet Archive,\footnote{\href{http://archive.org}{http://archive.org}} numerosas instituciones han trabajado en este área creando archivos de sitios web, unas veces generales y otras según el tema, idioma o país vinculado con el contenido. En 2003 se fundó el International Internet Preservation Consortium para facilitar la colaboración entre entidades interesadas en la preservación digital web (Niu, 2012). En un plano más informal, existen grupos de voluntarios como Archive Team\footnote{\href{http://archiveteam.org}{http://archiveteam.org}} dedicados a esta labor de preservación web de manera altruista, haciendo uso de herramientas libres y desarrollos propios. Ya nadie duda de la fugacidad de los contenidos en la red, se estima que la vida media de una página web es de 77 días,\footnote{\href{http://archive.org/about/faqs.php\#The_Wayback_Machine}{http://archive.org/about/faqs.php\#The\_Wayback\_Machine}} y de la importancia de preservar la web en su más amplio sentido para las generaciones futuras.

En el campo de la preservación digital es de máxima importancia almacenar el objeto en formatos que minimicen o impidan la pérdida de información. De la misma forma, los metadatos referentes a quién y cuándo generó el contenido son fundamentales para su contexto. Además, tanto el hardware que hace de soporte para almacenamiento, como el software y los formatos utilizados para la preservación deben contar con especificaciones públicas que aseguren que el acceso y recuperación de la información serán posibles en el futuro.

Con la explosión de contenidos generados en la red por los usuarios en los últimos años, de la mano de blogs, redes sociales y wikis, se hace más evidente la necesidad de llevar a cabo y coordinar la preservación de este subconjunto cada vez más prolífico de la World Wide Web. Instituciones como la Biblioteca del Congreso de Estados Unidos han mostrado su interés por estos contenidos, y en abril de 2010 hizo público un acuerdo con Twitter según el cual preservaría para la posteridad todos los tweets públicos desde 2006.\footnote{\href{http://blogs.loc.gov/loc/2010/04/how-tweet-it-is-library-acquires-entire-twitter-archive/}{http://blogs.loc.gov/loc/2010/04/how-tweet-it-is-library-acquires-entire-twitter-archive/}}

Desde la creación de Wikipedia en 2001, el uso de wikis como herramienta web para que los usuarios generen contenidos colaborativamente se ha disparado. Existen ya no solo una infinidad de wikis individuales sobre todo tipo de temas, sino también wikifarms que no son más que alojamientos web que integran miles de wikis y que en casos como Wikia sobrepasan los 250.000.[ref]

Todo este contenido wiki, imágenes y metadatos adjuntos con información sobre su autoría y evolución se encuentra en continuo riesgo de desaparición. Wikis que son abandonados por sus autores, administradores que descuidan el mantenimiento del servidor, dominios que caducan, ataques de vándalos y spammers o fallos de software y hardware, hacen que continuamente peligre la integridad de los datos. A pesar de que el contenido de los wikis se genera de manera colaborativa, la mayoría de ellos no ofrecen copias de seguridad completas y públicas del mismo a sus usuarios, haciendo impracticable que alguien pueda copiar manualmente los textos e imágenes si el wiki es de mediano tamaño.

Los wikis son un caso especial de contenido web, ya que además de los textos e imágenes, también disponen de historiales con todas las versiones anteriores de cada página. Este histórico y sus metadatos son de relevancia, no solo para mantener la información acerca de la autoría de los contenidos, sino de cara al estudio de la evolución de los textos y el comportamiento de la comunidad wiki. Asimismo, los textos están escritos una sintaxis que varía según cada motor wiki y que incluye una rica información sobre enlaces entre páginas, inserción de imágenes, estilo del texto y más..

Las iniciativas de preservación web existentes como Internet Archive o WebCitation no son capaces de almacenar estos historiales, metadatos o sintaxis wiki, ya que tratan las páginas del wiki como página web normales, almacenando el código HTML mostrado por el sitio y no el contenido original a partir del cual el servidor genera esta salida HTML. Por todo ello, actualmente la preservación de wikis se produce con muchas dificultades y severas omisiones que dan lugar a archivos incompletos y muy mejorables.

Además, muchos wikis se publican bajo algún tipo de licencia libre como GFDL o Creative Commons y sus variantes. Esto hace que no exista ningún problema legal a la hora de preservar los contenidos y redistribuir las copias, siempre que se haga sin fines lucrativos (para el caso de aquellas licencias que impidan el uso comercial).

Todo lo anterior, y la creciente cantidad de contenido wiki disponible en Internet, convierte la preservación de wikis en un problema abierto y con bastantes particularidades dentro del área de la preservación web, requiriendo de soluciones específicas y eficientes.

Para ello se fundó el proyecto WikiTeam en 2011. WikiTeam se constituye como un grupo de voluntarios que desarrolla herramientas de software para preservar wikis de manera colaborativa. Hasta el momento hemos creado una solución para wikis MediaWiki, cuyo uso ha permitido preservar más de 4.500 wikis,\footnote{\href{http://code.google.com/p/wikiteam/wiki/AvailableBackups}{http://code.google.com/p/wikiteam/wiki/AvailableBackups}} generando backups de sus textos, historiales, imágenes y metadatos. Los dumps generados han sido publicados en Internet Archive, creando una colección\footnote{\href{http://archive.org/details/wikiteam}{http://archive.org/details/wikiteam}} con un incalculable valor histórico y para investigación de comunidades wiki.

El resto del artículo se organiza de la siguiente forma. En la Sección 2 … En la Sección 3 presentamos WikiTeam. En la Sección 4... Terminamos con unas conclusiones y trabajo futuro.

[The remainder of this paper is organized as follows. In Section 2 .... In Section 3 we present WikiTeam, it success and . In Section 4. We finish we some conclusions and future work.]

\section{Preservación de wikis}

Ausencia de backups públicos de la mayoría de wikis (lo hacen Wikipedia, Wikia, Wikifur, Wikivoyage, y Citizendium y RationalWiki ofrecen el current; OSDev y OmegaWiki también ofrecen dumps). Diferenciar entre backups públicos y backups privados. Aquí hablamos de hacer dumps de todos los wikis sin pedir permiso, ya que ir pidiendolos uno a uno sería impracticable (habría admins que se opondrían a pesar de que sus sitios tengan lciencia libre, otros no sabrian o no querrían, y luego está el problema de las wikifarms donde sus usuarios están “vendidos” a si la empresa dueña del hosting quiere darles un backup (a veces aceptarán si pagan y otras ni eso)).

Comentar los scripts del chaval de WikiTravel %http://wikitravel.org/en/Wikitravel_talk:Database_dump
y este %http://code.google.com/p/oxygenpump/

Ejemplos de pérdidas de datos en wikis (ScribbleWiki)

Special:Export formulario nativo de MediaWiki para exportar lotes de páginas (impracticable para wikis de más de varios cientos de páginas; además generaría X ficheros .xml separados) y no guarda imágenes. Mirar si otros wikis tienen función de exportación PÚBLICA, DokuWiki tenía un plugin.

Urobe (se quedaron sin presupuesto)

Según el artículo de Urobe, almacenar el texto de un artículo de Wikipedia en sintaxis wiki ocupa el 10\% de la representación en HTML, y además al estar guardando el texto original no pierdes “cosas” que no se podrían extraer del HTML + imágenes (por ejemplo las fórmulas TeX que al servirlas se pasan a PNG).

En esta artículo se descargan wikis en HTML pero sin ánimo de conservarlos, solo para análisis %http://wikipapers.referata.com/wiki/Measuring_the_wikisphere

inexistencia de software para archivar wikis, la solución de archivar como una web normal hace que se pierdan los historiales, ejemplos de wikis que se hayan perdido por falta de backups, 

utilidad de los dumps para análisis comparativo con Wikipedia, más allá de la mera preservación

\section{WikiTeam preservando la wikiesfera}

WikiTeam\footnote{\href{http://code.google.com/p/wikiteam/}{http://code.google.com/p/wikiteam/}} es un grupo de voluntarios que desarrolla herramientas de preservación para wikis. Hasta ahora nos hemos centrado en los wikis que utilizan MediaWiki, pero tenemos previsto expandirnos a otros motores wiki.

Sus principales contribuciones son el desarrollo de un script llamado dumpgenerator.py que genera copias de seguridad de los contenidos de wikis MediaWiki, listados actualizados de URLs de wikis (tanto individuales como en wikifarms) y la creación de copias de seguridad por lotes usando los dos componentes anteriores. Siguiendo este método, hasta el momento se han preservado unos 4.500 wikis.

A ver si puedo sacar algunas estadísticas más de páginas, revisiones, usuarios, tamaño medio de los dumps, etc. Alguna tablita...

Posibilidad de compartir los dumps por BitTorrent (IA genera los .torrent desde hace poco).

\subsection{dumpgenerator.py: one wiki per run}

bien para tu wiki propio, pero se queda corto si quieres hacer backups de muchos

...

\subsection{launcher.py: wiki preservation in batches}

scrip simple para hacer backups de wikis a partir de listados, lanza muchos dumpgenerator.py, recomendación de partir la lista en trozos más pequeños y lanzar varias instancias en paralelo

problema, buscar una buena lista... encontramos la de Andrew Pavlo con 22000 wikis, que después de filtrarla y quedarnos con los que tenía API y estaban online todavía se redujo a 7000-8000 wikis

...

\section{Las 5 estrellas de la preservación de wikis}

As a result of our experience preserving and archiving wikis, we have learnt some tips that may help wiki communities to assure the indestructibility of their contents. It follows the Tim Berners-Lee 5 stars principles design for the Linked Open Data:

\begin{itemize}
\item \textbf{1 star:} Ofrecer de alguna forma los contenidos y los metadatos sobre autoría (fecha y autor de cada edición) aunque sea difícil de parsear.
\item \textbf{2 stars:} Que el motor usado disponga de un plugin o feature para exportar cada contenido (individualmente o en bloques) y sus metadatos de manera estructurada, quizás XML.
\item \textbf{3 stars:} En las estrellas anteriores el contenido podría tener licencias de “solo uso personal” o “solo educativo” y “sin lucro”. En 3 estrellas se pediría que el wiki tenga una licencia libre (que facilite la difusión y no ponga ataduras legales).
\item \textbf{4 stars:} ...
\item \textbf{5 stars:} el sumum creo que sería ofrecer tú mismo en descarga directa un backup completo de los contenidos y metadatos, de manera periódica y libre.
\end{itemize}

\section{Conclusiones y trabajo futuro}

%recalcar que nunca antes se habían publicado tantos “datasets” de wikis y que WikiTeam resuelve un problema grave, expandirnos a otros motores wiki, mejorar el proceso por lotes y la automatización del trabajo con grandes listas, más y mejores listas de wikis (el dichoso mapa) y como asegurar que son exhaustivas, finalmente desarrollar WikiEvidens para analizar los dumps bajados y comparar comportamiento de sus comunidades con la de Wikipedia,  

La cantidad de contenido generado por los usuarios, en particular wikis, disponible en la actualidad es enorme. Todo este texto, históricos, imágenes y metadatos se encuentran en grave peligro de desaparecer si no se toman acciones inmediatas para su preservación.

Las herramientas creadas por WikiTeam cubren un importante hueco existente en el área de la preservación digital de este tipo de sitios web, lo hacen maximizando el contenido y los metadatos recuperados para cada wiki y de una manera eficiente que permite el archivado por lotes de miles de sitios.

Los aspectos a mejorar de este software de preservación de wikis incluyen la expansión a otros motores, ya que en la actualidad se limita a MediaWiki, si bien es cierto que es el motor wiki más utilizado. Otro aspecto a mejorar serían las listas de wikis, asegurando que son exhaustivas y se mantienen actualizadas para de ese modo minimizar el número de wikis que quedan fuera de las tareas de preservación. Otras mejoras podrían ser añadir una interfaz gráfica sencilla que facilite el uso así como la traducción a otros idiomas de la herramienta.

Los backups de wikis generados tienen un gran valor histórico y bastante potencial como datasets para investigar el comportamiento de estas comunidades en línea. … wikievidens

\bibliographystyle{wink}        
\bibliography{wikiteam-2012}

\section*{Agradecimientos}

Agradecemos el trabajo realizado por los voluntarios de WikiTeam, desde los más activos hasta los esporádicos, que han ayudado reportando errores, enviando sugerencias, mejorando la documentación, haciendo pruebas y ejecutando los scripts para generar los miles de backups desde sus hogares o servidores.

\section*{Licencia}
Esta obra tiene licencia \href{http://creativecommons.org/licenses/by-sa/3.0/}{Creative Commons Reconocimiento-CompartirIgual 3.0 Unported}.

\end{document}
